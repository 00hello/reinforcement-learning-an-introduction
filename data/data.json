[{
  "id": "2_1",
  "title": "Figure 2.1: An exemplary bandit problem from the 10-armed testbed",
  "imageNames": ["2_1"]
}, {
  "id": "2_2",
  "title": "Figure 2.2: Average performance of epsilon-greedy action-value methods on the 10-armed testbed",
  "imageNames": ["2_2_1", "2_2_2"]
}, {
  "id": "2_3",
  "title": "Figure 2.3: Optimistic initial action-value estimates",
  "imageNames": ["2_3"]
}, {
  "id": "2_4",
  "title": "Figure 2.4: Average performance of UCB action selection on the 10-armed testbed",
  "imageNames": ["2_4"]
}, {
  "id": "2_5",
  "title": "Figure 2.5: Average performance of the gradient bandit algorithm",
  "imageNames": ["2_5"]
}, {
  "id": "2_6",
  "title": "Figure 2.6: A parameter study of the various bandit algorithms",
  "imageNames": ["2_6"]
}, {
  "id": "3_5",
  "title": "Figure 3.5: Grid example with random policy",
  "imageNames": ["3_5"]
}, {
  "id": "3_8",
  "title": "Figure 3.8: Optimal solutions to the gridworld example",
  "imageNames": ["3_8"]
}, {
  "id": "4_1",
  "title": "Figure 4.1: Convergence of iterative policy evaluation on a small gridworld",
  "imageNames": ["4_1"]
}, {
  "id": "4_2",
  "title": "Figure 4.2: Jack’s car rental problem",
  "imageNames": ["4_2_1", "4_2_2"]
}, {
  "id": "4_3",
  "title": "Figure 4.3: The solution to the gambler’s problem",
  "imageNames": ["4_3_1", "4_3_2"],
  "comments": ["There are many ties in the final policy, I simply break ties randomly"]
}, {
  "id": "5_1",
  "title": "Figure 5.1: Approximate state-value functions for the blackjack policy",
  "imageNames": ["5_1_1", "5_1_2", "5_1_3", "5_1_4"]
}, {
  "id": "5_3",
  "title": "Figure 5.3: The optimal policy and state-value function for blackjack found by Monte Carlo ES",
  "imageNames": ["5_3_1", "5_3_2", "5_3_3", "5_3_4"]
}, {
  "id": "5_4",
  "title": "Figure 5.4: Weighted importance sampling",
  "imageNames": ["5_4"]
}, {
  "id": "5_5",
  "title": "Figure 5.5: Ordinary importance sampling with surprisingly unstable estimates",
  "imageNames": ["5_5"]
}, {
  "id": "6_2",
  "title": "Figure 6.2: Random walk",
  "imageNames": ["6_2_1", "6_2_2"]
}, {
  "id": "6_3",
  "title": "Figure 6.3: Batch updating",
  "imageNames": ["6_3"]
}, {
  "id": "6_4",
  "title": "Figure 6.4: Sarsa applied to windy grid world",
  "imageNames": ["6_4"]
}, {
  "id": "6_5",
  "title": "Figure 6.5: The cliff-walking task",
  "imageNames": ["6_5"]
}, {
  "id": "6_7",
  "title": "Figure 6.7: Interim and asymptotic performance of TD control methods",
  "imageNames": ["6_7"]
}, {
  "id": "6_8",
  "title": "Figure 6.8: Comparison of Q-learning and Double Q-learning",
  "imageNames": ["6_8"]
}, {
  "id": "7_2",
  "title": "Figure 7.2: Performance of n-step TD methods on 19-state random walk",
  "imageNames": ["7_2"]
}, {
  "id": "8_3",
  "title": "Figure 8.3: Average learning curves for Dyna-Q agents varying in their number of planning steps",
  "imageNames": ["8_3"]
}, {
  "id": "8_5",
  "title": "Figure 8.5: Average performance of Dyna agents on a blocking task",
  "imageNames": ["8_5"]
}, {
  "id": "8_6",
  "title": "Figure 8.6: Average performance of Dyna agents on a shortcut task",
  "imageNames": ["8_6"]
}, {
  "id": "8_7",
  "title": "Figure 8.7: Prioritized sweeping significantly shortens learning time on the Dyna maze task",
  "imageNames": ["8_7"]
}, {
  "id": "9_1",
  "title": "Figure 9.1: Gradient Monte Carlo algorithm on the 1000-state random walk task",
  "imageNames": ["9_1_1", "9_1_2"]
}, {
  "id": "9_2",
  "title": "Figure 9.2: Semi-gradient n-steps TD algorithm on the 1000-state random walk task",
  "imageNames": ["9_2_1", "9_2_2"]
}, {
  "id": "9_5",
  "title": "Figure 9.5: Fourier basis vs polynomials on the 1000-state random walk task",
  "imageNames": ["9_5"]
}, {
  "id": "9_8",
  "title": "Figure 9.8: Example of feature width’s effect on initial generalization and asymptotic accuracy",
  "imageNames": ["9_8_1", "9_8_2", "9_8_3", "9_8_4", "9_8_5"]
}, {
  "id": "9_10",
  "title": "Figure 9.10: Single tiling and multiple tilings on the 1000-state random walk task",
  "imageNames": ["9_10"]
}, {
  "id": "10_1",
  "title": "Figure 10.1: The cost-to-go function for Mountain Car task in one run",
  "imageNames": ["10_1_1", "10_1_2", "10_1_3"]
}, {
  "id": "10_2",
  "title": "Figure 10.2: Learning curves for semi-gradient Sarsa on Mountain Car task",
  "imageNames": ["10_2"]
}, {
  "id": "10_3",
  "title": "Figure 10.3: One-step vs multi-step performance of semi-gradient Sarsa on the Mountain Cartask",
  "imageNames": ["10_3"]
}, {
  "id": "10_4",
  "title": "Figure 10.4: Effect of the alpha and n on early performance of n-step semi-gradient Sarsa",
  "imageNames": ["10_4"]
}, {
  "id": "10_5",
  "title": "Figure 10.5: Differential semi-gradient Sarsa on the access-control queuing task",
  "imageNames": ["10_5_1", "10_5_2"]
}, {
  "id": "11_2",
  "title": "Figure 11.2: Baird's Counterexample",
  "imageNames": ["11_2"]
}, {
  "id": "12_3",
  "title": "Figure 12.3: Off-line λ-return algorithm on 19-state random walk",
  "imageNames": ["12_3"]
}, {
  "id": "12_6",
  "title": "Figure 12.6: TD(λ) algorithm on 19-state random walk",
  "imageNames": ["12_6"]
}, {
  "id": "12_7",
  "title": "Figure 12.7: True online TD(λ) algorithm on 19-state random walk",
  "imageNames": ["12_7"]
}, {
  "id": "E_1",
  "title": "Return Specific Importance Sampling",
  "imageNames": ["E_1_1", "E_1_2", "E_1_3", "E_1_4", "E_1_5", "E_1_6", "E_1_7"],
  "comments": [
    "This example is adapted from example 5.5, Infinite Variance",
    "I made following modifications to the original example to demonstrate the advantage of return specific importance sampling",
    "1. discount now is 0.5 instead of 1",
    "2. the transition from state s to state s will have reward +2",
    "3. the action BACK will lead to state s or terminal state with equal probability",
    "In this setting, the true state value is 2",
    "PDWIS is biased and inconsistent, so theoretically it doesn't work",
    "CPDWIS is still biased however consistent",
    "Reference: http://psthomas.com/papers/Thomas2015c.pdf (3.9 3.10)"
  ]
}, {
  "id": "E_2",
  "title": "N-Step Tree Backup & N-Step Q(σ)",
  "imageNames": ["E_2"],
  "comments": [
    "This example is Windy Grid World",
    "Target policy is greedy, behavior policy is epsilon-greedy",
    "I use a small alpha (step size), although in deterministic environment, alpha = 1 is optimal",
    "epsilon = 0.2, alpha = 0.1"
  ]
}, {
  "id": "E_3",
  "title": "Sum of TD errors",
  "imageNames": ["E_3"],
  "comments": [
    "See SumOfTDErrors.pdf for more details"
  ]
}]